{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXt2LEBAyW3x+L1vJibv+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dusarp/deep-learning-experiments/blob/main/basic_segmentation_unet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-net"
      ],
      "metadata": {
        "id": "wC4I1Vb3B0gt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiaX1PfeBmRX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = self.conv_block(in_channels, 64)\n",
        "        self.encoder2 = self.conv_block(64, 128)\n",
        "        self.encoder3 = self.conv_block(128, 256)\n",
        "        self.encoder4 = self.conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = self.upconv_block(1024, 512)\n",
        "        self.decoder3 = self.upconv_block(512, 256)\n",
        "        self.decoder2 = self.upconv_block(256, 128)\n",
        "        self.decoder1 = self.upconv_block(128, 64)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(nn.MaxPool2d(kernel_size=2)(enc1))\n",
        "        enc3 = self.encoder3(nn.MaxPool2d(kernel_size=2)(enc2))\n",
        "        enc4 = self.encoder4(nn.MaxPool2d(kernel_size=2)(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(nn.MaxPool2d(kernel_size=2)(enc4))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.decoder4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)  # Skip connection\n",
        "        dec3 = self.decoder3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)  # Skip connection\n",
        "        dec2 = self.decoder2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)  # Skip connection\n",
        "        dec1 = self.decoder1(dec2)\n",
        "\n",
        "        return self.final_conv(dec1)\n",
        "\n",
        "# Example usage:\n",
        "model = UNet(in_channels=3, out_channels=1)  # For binary segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "bVVbu7UMC9QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_dir, masks_dir):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.images = os.listdir(images_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.images[idx].replace('.jpg', '_mask.png'))\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # Assuming mask is grayscale\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        image = transform(image)\n",
        "        mask = transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Example usage:\n",
        "dataset = SegmentationDataset(images_dir='path/to/images', masks_dir='path/to/masks')"
      ],
      "metadata": {
        "id": "YDR4SdBWC85A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "Ko69KliqDF50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, dataset):\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Change based on your output classes\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(10):  # Number of epochs\n",
        "        for images, masks in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.squeeze(), masks)  # Adjust based on output shape\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Example usage:\n",
        "train_model(model=model, dataset=dataset)"
      ],
      "metadata": {
        "id": "gToW8RdqC82-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "ZNeh9dI0DN2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_image.unsqueeze(0))  # Add batch dimension\n",
        "    return output.squeeze().numpy()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming input_image is a preprocessed image tensor of shape (C,H,W)\n",
        "predicted_mask = predict(model=model, input_image=input_image)"
      ],
      "metadata": {
        "id": "yizL4af7C81C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}